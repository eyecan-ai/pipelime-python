{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelime Samples And Sequences\n",
    "\n",
    "At the core of pipelime's dataset management we find the concept of samples sequences.\n",
    "All the functionalities you need are packed into the SamplesSequence class, where the\n",
    "operational methods, eg, `shuffle`, `sort`, etc, are **dynamically** generated from\n",
    "internal and external definitions (more on this later). Therefore, **you won't find them\n",
    "by just looking at the code**. Instead, you can list them from the command line using\n",
    "`pipelime list` or explicitly calling the printer from an interactive session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipelime.cli.utils import print_sequence_operators_list\n",
    "\n",
    "print_sequence_operators_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, we have two kind of sequence operations:\n",
    "1. **generators**: class-methods that generate a sequence of samples\n",
    "1. **pipes**: instance-methods that append an operation to the sequence\n",
    "\n",
    "Note that *piped operations* follow the general pipelime approach of *returning* a new\n",
    "object leaving the original unchanged. Usually, it does not add a significant overhead.\n",
    "\n",
    "To get the list of arguments required for a given operation, just use the usual\n",
    "`pipelime help <operation>` from command line or the\n",
    "`pipelime.cli.utils.print_command_or_op_info` python function. For example, let's see\n",
    "how to load an underfolder dataset and shuffle it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipelime.cli.utils import print_command_op_stage_info\n",
    "\n",
    "print_command_op_stage_info(\"from_underfolder\")\n",
    "print_command_op_stage_info(\"shuffle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Data Pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipelime.sequences import SamplesSequence\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "seq = SamplesSequence.from_underfolder(  # type: ignore\n",
    "    \"../../tests/sample_data/datasets/underfolder_minimnist\",\n",
    "    # merge_root_items=False,  # see below about toggling this comment\n",
    ")\n",
    "print(\"Before shuffling:\", flush=True)\n",
    "display(Image.fromarray(seq[0]['image']()))\n",
    "\n",
    "seq = seq.shuffle(seed=42)[2:6]\n",
    "print(\"After shuffling and slicing:\", flush=True)\n",
    "display(Image.fromarray(seq[0]['image']()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above we have also accessed a sample by its index and an item by its\n",
    "name. Note that what you get from `seq[0]['image']` is a `pipelime.items.Item` object,\n",
    "so to get its value you have to `__call__()` it. Then, where the actual data come from may\n",
    "vary:\n",
    "1. first, if the data has been already loaded and cached, the cached value is returned\n",
    "1. then, all file sources are checked\n",
    "1. finally, remote data lakes are accessed\n",
    "\n",
    "The usual approach is to create a new object every time the data is changed,\n",
    "so **you should not really care where such data come from**. Though, to reduce memory\n",
    "footprint, you can disable item data caching by setting the `Item.cache_data` property\n",
    "or using the `pipelime.items.no_data_cache` context manager:\n",
    "\n",
    "```\n",
    "    # disable data cache for all items\n",
    "    with no_data_cache():\n",
    "        ...\n",
    "    \n",
    "    # disable only for BinaryItem and NumpyItem\n",
    "    with no_data_cache(BinaryItem, NumpyItem):\n",
    "        ...\n",
    "    \n",
    "    # apply at function invocation\n",
    "    @no_data_cache(ImageItem)\n",
    "    def my_fn():\n",
    "        ...\n",
    "```\n",
    "\n",
    "Now let's see what happens when we write the above sequence to disk. The writer is,\n",
    "indeed, just another operation, so we append it to the sequence and then just iterate\n",
    "over the sequence to write the samples to disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = seq.to_underfolder(\"./writer_output\", exists_ok=True)\n",
    "for _ in writer:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the items know how to serialize themselves since they represent specific\n",
    "data formats. To see all the supported formats, just inquire the item factory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipelime.items import Item\n",
    "from rich.pretty import pprint\n",
    "\n",
    "pprint(Item.ITEM_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "What if we want to distribute the computation over multiple cores? Just use a **Grabber**!\n",
    "\n",
    "**NB**: *Multiprocessing does not work in Jupyter notebooks, so we have packed the logic\n",
    "in grabber_example.py*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python grabber_example.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting thing to note is that when we write an item to disk, such file is,\n",
    "indeed, a new *file source* for the item. Since no changes are made to the actual value,\n",
    "this data source is added to the same item instance we initially loaded<sup>1</sup>:\n",
    "\n",
    "<sup>[1] *The attentive reader will notice that if we had set `merge_root_items=False`\n",
    "when loading the underfolder dataset above, even the Sample object would have been the\n",
    "SAME instance.*</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original Sample:\", flush=True)\n",
    "print(seq[0])\n",
    "\n",
    "print(\"Written Sample:\", flush=True)\n",
    "print(writer[0])\n",
    "\n",
    "for org_sample, wrt_sample in zip(seq, writer):\n",
    "    for v1, v2 in zip(org_sample, wrt_sample):\n",
    "        assert v1 is v2\n",
    "    \n",
    "    ## This is true if we set `merge_root_items=False`\n",
    "    # assert org_sample is wrt_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `pipe` Command\n",
    "\n",
    "Once we have a sequence of operations, we can serialize it and replay it later through\n",
    "the `pipe` command. To this end, first we ask the samples sequence to serialize itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "print(yaml.safe_dump(writer.to_pipe(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can create a config file following the `pipe` syntax and copy-pasting the\n",
    "serialized sequence above, though removing the `from_underfolder` and `to_underfolder`\n",
    "steps, since `pipe` already takes care of that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pipelime pipe --config pipe_cfg.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "seq1 = SamplesSequence.from_underfolder(\"writer_output\")  # type: ignore\n",
    "seq2 = SamplesSequence.from_underfolder(\"writer_output_piped\")  # type: ignore\n",
    "assert len(seq1) == len(seq2)\n",
    "for s1, s2 in zip(seq1, seq2):\n",
    "    assert list(s1.keys()) == list(s2.keys())\n",
    "    for k in s1.keys():\n",
    "        if isinstance(s1[k](), np.ndarray):\n",
    "            assert np.array_equal(s1[k](), s2[k](), equal_nan=True)\n",
    "        else:\n",
    "            assert s1[k]() == s2[k]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data Caching\n",
    "\n",
    "Pipelime provides a special operator `cache` to serialize to disk a whole sample the\n",
    "first time it is accessed and then load it from disk the next time, instead of\n",
    "triggering again the whole source data pipeline. This is really useful when:\n",
    "* you loop over the data multiple times\n",
    "* the data processing is time consuming but fixed, ie, for each index you always get the\n",
    "same sample\n",
    "\n",
    "To show how it works, we will in fact add some randomness into the pipeline, so that we\n",
    "can clearly see the difference between *caching* and *no caching*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipelime.sequences import SamplesSequence, Sample\n",
    "from pipelime.stages import SampleStage\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "\n",
    "def _print_label(data_seq, idx = 0, times = 3):\n",
    "    for i in range(times):\n",
    "        print(f\"Reading label #{idx} ({i})\", data_seq[idx][\"label\"](), flush=True)\n",
    "\n",
    "\n",
    "class RandomNoiseStage(SampleStage):\n",
    "    def __call__(self, x: Sample):\n",
    "        return x.set_value(\"label\", x[\"label\"]() + np.random.normal(0, .1))  # type: ignore\n",
    "\n",
    "seq = SamplesSequence.from_underfolder(  # type: ignore\n",
    "    \"../../tests/sample_data/datasets/underfolder_minimnist\"\n",
    ").map(RandomNoiseStage())\n",
    "\n",
    "print(\"Every time we read a sample, the label is modified by a random noise:\", flush=True)\n",
    "_print_label(seq)\n",
    "\n",
    "print(\"\\nInstead, a cached sequence always return the first value we get:\", flush=True)\n",
    "shutil.rmtree(\"local_cache\", ignore_errors=True)\n",
    "cached_seq = seq.cache(\"local_cache\")\n",
    "_print_label(cached_seq)\n",
    "\n",
    "print(\"\\nWe can even re-use the same cached data between different runs:\", flush=True)\n",
    "another_cached_seq = seq.cache(\"local_cache\", reuse_cache=True)\n",
    "_print_label(another_cached_seq)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "469fef48c6b116a8cbba19e3056aac05c6680f70f24ce841e82a3d44c42603cf"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pl38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
