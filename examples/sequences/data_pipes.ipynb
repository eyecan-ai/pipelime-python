{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelime Samples And Sequences\n",
    "\n",
    "At the core of pipelime's dataset management we find the concept of samples sequences.\n",
    "All the functionalities you need are packed into the SamplesSequence class, where the\n",
    "operational methods, eg, `suffle`, `slice`, etc, are **dynamically** generated from\n",
    "internal and external definitions (more on this later). Therefore, **you won't find them\n",
    "by just looking at the code**. Instead, you can list them from the command line using\n",
    "`pipelime list` or explicitly calling the printer from an interactive session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipelime.cli.utils import print_sequence_operators_list\n",
    "\n",
    "print_sequence_operators_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, we have two kind of sequence operations:\n",
    "1. **generators**: class-methods that generate a sequence of samples\n",
    "1. **pipes**: instance-methods that append an operation to the sequence\n",
    "\n",
    "Note that *piped operations* follow the general pipelime approach of *returning* a new\n",
    "object leaving the original unchanged. Usually, it does not add a significant overhead.\n",
    "\n",
    "To get the list of arguments required for a given operation, just use the usual\n",
    "`pipelime help <operation>` from command line or the\n",
    "`pipelime.cli.utils.print_command_or_op_info` python function. For example, let's see\n",
    "how to load an underfolder dataset and shuffle it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipelime.cli.utils import print_command_op_stage_info\n",
    "\n",
    "print_command_op_stage_info(\"from_underfolder\")\n",
    "print_command_op_stage_info(\"shuffle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Data Pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipelime.sequences import SamplesSequence\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "seq = SamplesSequence.from_underfolder(  # type: ignore\n",
    "    \"../../tests/sample_data/datasets/underfolder_minimnist\"\n",
    ")\n",
    "print(\"Before shuffling:\", flush=True)\n",
    "display(Image.fromarray(seq[0]['image']()))\n",
    "\n",
    "seq = seq.shuffle(seed=42)\n",
    "print(\"After shuffling:\", flush=True)\n",
    "display(Image.fromarray(seq[0]['image']()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous code we have also accessed a sample by its index and an item by its\n",
    "name. Note that what you get from `seq[0]['image']` is a `pipelime.items.Item` object,\n",
    "so to get its value you have to `__call__()` it. Then, where the actual data come from may\n",
    "vary:\n",
    "1. first, if the data has been already loaded and cached, the cached value is returned\n",
    "1. then, all file sources are checked\n",
    "1. finally, remote data lakes are accessed\n",
    "\n",
    "Note that the usual approach is to create a new object every time the data is changed,\n",
    "so **you should not really care where such data come from**. Though, to reduce memory\n",
    "footprint, you can disable item data caching by setting the `Item.cache_data` property\n",
    "or using the `pipelime.items.no_data_cache` context manager (NB: can be used as function\n",
    "decorator as well!).\n",
    "\n",
    "Now let's see what happens when we write the above sequence to disk. The writer is,\n",
    "indeed, just another operation, so we append it to the sequence and then just iterate\n",
    "over the sequence to write the samples to disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = seq.to_underfolder(\"./writer_output\", exists_ok=True)\n",
    "for _ in writer:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to distribute the computation over multiple cores? Just use a Grabber!\n",
    "\n",
    "**NB**: *Multiprocessing does not work in Jupyter notebooks, so we have packed the logic\n",
    "in grabber_example.py*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python grabber_example.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting thing to note is that when we write an item to disk, such file is,\n",
    "indeed, a new *file source* for the item. Since no changes are made to the actual value,\n",
    "this data source is added to the same item instance we initially loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original Sample:\", flush=True)\n",
    "print(seq[0])\n",
    "\n",
    "print(\"Written Sample:\", flush=True)\n",
    "print(writer[0])\n",
    "\n",
    "for org_sample, wrt_sample in zip(seq, writer):\n",
    "    assert org_sample is wrt_sample"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "469fef48c6b116a8cbba19e3056aac05c6680f70f24ce841e82a3d44c42603cf"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pl38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
