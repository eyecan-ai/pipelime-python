{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelime Command Line Interface\n",
    "\n",
    "The pipelime command line interface is a powerful tool to automate data processing.\n",
    "First, you can get help simply typing `pipelime`, `pipelime help`, `pipelime --help` or\n",
    "even `pipelime -h`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pipelime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CLI is built around the concept of `Pipelime Command`, which encapsulates an\n",
    "operation and makes it available both to the CLI and usual python scripting. Such\n",
    "commands are dynamically loaded at runtime, so you can always run a third-party command\n",
    "just by setting its full class path, eg, `my_package.my_module.MyCommand` or\n",
    "`path/to/my_module.py:MyCommand`. Alternatively, let pipelime find and load your command\n",
    "by setting `--module my_package.my_module` or `--module path/to/my_module.py`, then\n",
    "refer to it by its pydantic title (see **#TODO REF**).\n",
    "\n",
    "The list of available commands and sequence operators (more on this later in this doc)\n",
    "can be retrieved with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pipelime list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to get help for a specific command or sequence operator, just type\n",
    "`pipelime help <cmd>`, `pipelime <cmd> help`, `pipelime --help <cmd>`, etc,\n",
    "eg (best viewed in a *real* terminal window):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pipelime --help clone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, the same help can be printed during an interactive session by explicitly calling the printer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipelime.cli.utils import print_command_or_op_info\n",
    "\n",
    "print_command_or_op_info(\"clone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running A Command\n",
    "\n",
    "As you can see above, the *clone* command:\n",
    "* needs 3 arguments: **input** (required), **output** (required) and **grabber** (optional)\n",
    "* each argument is, in fact, an *interface* encapsulating a full range of options in a tree-like structure\n",
    "\n",
    "When you call *clone* through the pipelime cli, you can set all those options in different ways, ie:\n",
    "* pydash-like key paths prefixed with \"+\", where the \".\" separates nested keys and \"[]\"\n",
    "indexes a list, eg, `+input.folder path/to/folder`.\n",
    "* a json/yaml configuration file specified as `--config path/to/cfg.yaml`. Note that command line\n",
    "options update and override config file definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pipelime clone +input.folder ../../tests/sample_data/datasets/underfolder_minimnist +output.folder ./clone_out +output.exists_ok=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, the `CloneCommand` can be created and run in a python script as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipelime.commands import CloneCommand\n",
    "from pipelime.cli.pretty_print import print_command_outputs\n",
    "\n",
    "cmd = CloneCommand(\n",
    "    input={\"folder\": \"../../tests/sample_data/datasets/underfolder_minimnist\"},  # type: ignore\n",
    "    output={\"folder\": \"./clone_out\", \"exists_ok\": True},  # type: ignore\n",
    ")\n",
    "cmd()\n",
    "print_command_outputs(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing A Graph Of Commands\n",
    "\n",
    "Multiple commands can be chained ad executed as a Direct Acyclic Graph (DAG) by the *run* command (`RunCommand`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_command_or_op_info(\"run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `nodes` attribute is a mapping of nodes, where the keys are the nodes' names and the values the actual commands to execute. As a practical example, look at the *complex_dag.yaml* file. It may seem intimidating at first, but we can easily understand the data flow by drawing it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pipelime draw --config complex_dag.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops! Something went wrong...\n",
    "As the error message says, we need to specify some variables. To get a full list,\n",
    "just audit the configuration file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pipelime audit --config complex_dag.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `complex_params.yaml` file defines such variables, except for `params.root_folder`, which defined by the user on the command line using the special `!` prefix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pipelime audit --config complex_dag.yaml --context complex_params.yaml !params.root_folder=./output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to inspect and run the computation graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pipelime draw --config complex_dag.yaml --context complex_params.yaml !params.root_folder=./output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pipelime run --config complex_dag.yaml --context complex_params.yaml !params.root_folder=./output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now run again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pipelime run --config complex_dag.yaml --context complex_params.yaml !params.root_folder=./output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahaa! We got an error:\n",
    "```\n",
    "FileExistsError: Trying to overwrite an existing dataset. Please use `exists_ok=True` to overwrite.\n",
    "```\n",
    "Looking at `complex_dag.yaml`, we can see that the `exists_ok` option is not set for\n",
    "`nodes.sum_1.$args.output`. We can fix this by adding it on the command line (best viewed in a *real* terminal window):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pipelime run --config complex_dag.yaml --context complex_params.yaml !params.root_folder=./output \"+nodes.sum_1.$args.output.exists_ok\" True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Parameter Interfaces\n",
    "\n",
    "Pipelime commands are built for best modularity and reusability, so common parameters,\n",
    "such as input and output datasets, are defined in standard interfaces. Here a brief\n",
    "overview."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Dataset And Schema Validation\n",
    "\n",
    "An input dataset is defined by:\n",
    "* `folder`: the root folder of the dataset\n",
    "* `merge_root_items`: wether shared root items should be added to each sample\n",
    "* `schema`: the optional schema validation\n",
    "\n",
    "The validation schema is mainly defined by the `schema.sample_schema` key, which is\n",
    "basically a mapping from sample keys to item type, eg:\n",
    "```\n",
    "{\n",
    "    \"schema\": {\n",
    "        \"sample_schema\": {\n",
    "            \"image\": {\n",
    "                \"class_path\": \"ImageItem\",\n",
    "                \"is_optional\": False,\n",
    "            },\n",
    "            \"label\": {\n",
    "                \"class_path\": \"TxtNumpyItem\",\n",
    "                \"is_optional\": True,\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "Fine-grained validation can be performed by adding validator callables or, instead of\n",
    "the dictionary above, a full-fledged pydantic model, eg:\n",
    "```\n",
    "$ my_schema.py\n",
    "----------------------------------------------------\n",
    "from pydantic import BaseModel, validator\n",
    "from pipelime.items import ImageItem, TxtNumpyItem\n",
    "from typing import Optional\n",
    "\n",
    "class MySchema(BaseModel):\n",
    "    image: ImageItem\n",
    "    label: Optional[TxtNumpyItem] = None\n",
    "\n",
    "    @validator(\"image\")\n",
    "    def check_image_size(cls, v):\n",
    "        if v.shape[0] != 224 or v.shape[1] != 224:\n",
    "            raise ValueError(\"Image must be 224x224\")\n",
    "        return v\n",
    "```\n",
    "Then:\n",
    "```\n",
    "{\n",
    "    \"schema\": {\n",
    "        \"sample_schema\": \"my_schema.py:MySchema\"\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**TIP**: to get a minimal schema for an existing dataset, try the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pipelime validate +input.folder ../../tests/sample_data/datasets/underfolder_minimnist +max_samples 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Dataset And Serialization Modes\n",
    "\n",
    "Like input datasets, an output dataset is mainly defined by the `folder` path and an\n",
    "optional `schema` definition. Moreover, you can fine-tune how items are actually\n",
    "serialized to disk.\n",
    "\n",
    "The standard serialization procedure tries the following sequence of actions and stops\n",
    "when one of them succeeds:\n",
    "1. *remote file* (`REMOTE_FILE`): if remote source addresses are available, they are dumped\n",
    "1. *hard link* (`HARD_LINK`): if file sources are available, one of them is hard linked\n",
    "1. *deep copy* (`DEEP_COPY`): if file sources are available, one of them is copy\n",
    "1. *new file* (`CREATE_NEW_FILE`): a new file is created by serializing the item value\n",
    "\n",
    "Moreover, a *soft link* (`SYM_LINK`) option can be tried instead of *remote file* and\n",
    "*hard link*, but only if **explicitly requested** (see below).\n",
    "\n",
    "To alter this behavior, you can set the `serialization` option so as to override,\n",
    "disable or force the desired mode, eg:\n",
    "```\n",
    "{\n",
    "    \"serialization\": {\n",
    "        \"override\": {\n",
    "            \"CREATE_NEW_FILE\": [\"PngImageItem\", \"BinaryItem\"]\n",
    "        }\n",
    "        \"disable\": {\n",
    "            \"NumpyItem\": \"HARD_LINK\"\n",
    "        }\n",
    "        \"keys\": {\n",
    "            \"mask\": \"REMOTE_FILE\",\n",
    "            \"label\": \"SYM_LINK\",\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Grabber\n",
    "\n",
    "The grabber interface provides a easy way to distribute data iteration on multiple\n",
    "processes. The whole pipeline, included the item serialization, is executed in parallel,\n",
    "but be aware that the process spawning overhead can be significant, so parameters should\n",
    "be carefully tuned. The definition is as follows:\n",
    "* `num_workers`: the number of processes to spawn. If negative, the number of (logical)\n",
    "cpu cores is used.\n",
    "* `prefetch`: the number of samples loaded in advanced by each worker, which might be\n",
    "useful if the parent command has to post-process the data."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "469fef48c6b116a8cbba19e3056aac05c6680f70f24ce841e82a3d44c42603cf"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pl38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
